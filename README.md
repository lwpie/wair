## TLDR: Steps to reproduce

0. Register an AWS account with the Tokyo (ap-northeast-1) region enabled.
1. Create a [Gateway Endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html) and (optional) VPC for S3 Express One.
2. Launch a c7gd.8xlarge instance in AZ1 (ap-northeast-1c) running `Debian 12`.
3. Install dependencies (apt) `redis`, [`libparquet-dev`](https://arrow.apache.org/install/), (pip) `duckdb`, `redis`, `parse`, `tqdm`, (compile) [DuckDB](https://duckdb.org/docs/stable/dev/building/overview.html) and `g++ split.cc -o split $(pkg-config --cflags --libs parquet) -fopenmp -O3`. 
4. Configure the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) and replace the API keys in `nr/h`.
5. Generate the `tpch` folder in the `exec` folder using `dbgen.sh`.
6. Start the pipeline with `zsh pipeline.sh > >(tee logs.txt) 2> >(tee errs.txt)`
7. Copy `logs.txt` and `errs.txt` into `stats` folder, and run `python3 analyze.py stats` to see results.

If you have any questions or need further information, feel free to ask. I recommend running step 5 on a local server with large memory. The AWS region, availability zone, and other system configurations can be modified according to your preferences if you're familiar with Linux and AWS.


This repository contains implementations associated with our academic paper. It is organized as follows:

1. **duckdb**: Modifications to DuckDB.
2. **dbgen**: TPC-H benchmark data generation.
3. **exec**: Workflow pipeline components and pseudo-distributed scripts for simplified execution.
4. **logs**: Experiment results and associated plotting scripts.

### duckdb

We mainly modify the pipeline executor and query profiler, while also patching the HTTPFS and Parquet extension to collect detailed I/O information. For additional details, please refer to the git commit logs. When JSON profiling is enabled, the collected statistics are stored in:

1. `httpfs_stats.txt`: I/O transfer size per file.
2. `file_stats.txt`: Result set size per column per file.

### dbgen

The TPC-H dataset is generated using DuckDB's TPC-H Extension. Each ingestion batch is created separately using `dbgen.sh`, then modified through `dbgen.sql` to align with our experimental settings. Random query batches are generated by `query.py`. The dataset and queries fully adhere to the TPC-H specification.

### exec

Experiments are executed in a pipeline, with queries and reclustering running from batch 25 to 50. The `lineitem` table is stored in S3, and metadata is maintained in a local Redis server. Metadata ingestion and pruning are handled by `meta.py`, while `wair.py` contains our WAIR implementation. The `diff` folder and `diff.sh` provide scripts to verify the correctness of our modifications to DuckDB. The `legacy` folder offers an alternate implementation with minimal modifications for debugging purposes and produces the same results.

### logs

`logs/33/2` and `logs/33/6` contain the TPC-H W(2) and W(6) results respectively. `analyze.py` provides a quick plain text analysis of specific experiment results. Other Python scripts and `plot.s`h are used to generate the figures in our paper.
