This repository contains implementations associated with our academic paper. It is organized as follows:

1. **duckdb**: Modifications to DuckDB.
2. **dbgen**: TPC-H benchmark data generation.
3. **exec**: Workflow pipeline components and pseudo-distributed scripts for simplified execution.
4. **logs**: Experiment results and associated plotting scripts.

### duckdb

We mainly modify the pipeline executor and query profiler, while also patching the HTTPFS and Parquet extension to collect detailed I/O information. For additional details, please refer to the git commit logs. When JSON profiling is enabled, the collected statistics are stored in:

1. `httpfs_stats.txt`: I/O transfer size per file.
2. `file_stats.txt`: Result set size per column per file.

### dbgen

The TPC-H dataset is generated using DuckDB's TPC-H Extension. Each ingestion batch is created separately using `dbgen.sh`, then modified through `dbgen.sql` to align with our experimental settings. Random query batches are generated by `query.py`. The dataset and queries fully adhere to the TPC-H specification.

### exec

Experiments are executed in a pipeline, with queries and reclustering running from batch 25 to 50. The `lineitem` table is stored in S3, and metadata is maintained in a local Redis server. Metadata ingestion and pruning are handled by `meta.py`, while `wair.py` contains our WAIR implementation. The `diff` folder and `diff.sh` provide scripts to verify the correctness of our modifications to DuckDB. The `legacy` folder offers an alternate implementation with minimal modifications for debugging purposes and produces the same results.

### logs

`logs/33/2` and `logs/33/6` contain the TPC-H W(2) and W(6) results respectively. `analyze.py` provides a quick plain text analysis of specific experiment results. Other Python scripts and `plot.s`h are used to generate the figures in our paper.


If you have any questions or need further information, feel free to ask.